{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\anaconda3\\lib\\site-packages\\torchtext\\datasets\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\caspe\\anaconda3\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\caspe\\anaconda3\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\caspe\\anaconda3\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Weird deprecation issues in torchtext means this cell needs to be run twice\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.vocab import vocab\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')\n",
    "train_dataset, val_dataset = random_split(list(train_dataset),[20000,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement tokenizer to remove unwanted tokens\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>','',text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text.lower())\n",
    "    text = re.sub('[\\W]+',' ', text.lower() + ' '.join(emoticons).replace('-',''))\n",
    "    tokenized = text.split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 69000\n"
     ]
    }
   ],
   "source": [
    "token_counts = Counter()\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "print(f'Vocab size: {len(token_counts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vocabulary from Counter in above cell and add tokens: <pad>,<unk>\n",
    "sorted_tuples = sorted(token_counts.items(),key=lambda x: x[1],reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token('<pad>',0)\n",
    "vocab.insert_token('<unk>',1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function to load batches\n",
    "#   - Tokenize each sentence\n",
    "#   - Extract target label\n",
    "#   - Pad with zeroes\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1.0 if x == 2 else 0.0\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list,text_list,lengths = [],[],[]\n",
    "    for _label,_text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text),dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list).to(device)\n",
    "    lengths = torch.tensor(lengths).to(device)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,batch_first=True).to(device)\n",
    "    return padded_text_list,label_list,lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=collate_batch)\n",
    "val_dl = DataLoader(val_dataset,batch_size=batch_size,shuffle=False,collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,rnn_hidden_size,fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_dim,padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim,rnn_hidden_size,batch_first=True,bidirectional=True)\n",
    "        # self.rnn = nn.LSTM(embed_dim,rnn_hidden_size,batch_first=True)\n",
    "        # self.rnn = nn.RNN(embed_dim,rnn_hidden_size,batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2,fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,text,lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out,lengths.cpu().numpy(),enforce_sorted=False,batch_first=True)\n",
    "        out, (hidden,cell) = self.rnn(out)\n",
    "        # _,hidden = self.rnn(out)\n",
    "        # out = hidden[-1,:,:] # Extract last state\n",
    "        out = torch.concat((hidden[-2,:,:],hidden[-1,:,:]),dim=1) # Extract last state\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.sigmoid(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "model = RNN(vocab_size,embed_dim,rnn_hidden_size,fc_hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0,0\n",
    "    for text_batch,label_batch,lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(text_batch,lengths)[:,0]\n",
    "        loss = loss_fn(preds,label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((preds >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0,0\n",
    "    with torch.no_grad():\n",
    "        for text_batch,label_batch,lengths in dataloader:\n",
    "            preds = model(text_batch,lengths)[:,0]\n",
    "            loss = loss_fn(preds,label_batch)\n",
    "            total_acc += ((preds >= 0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Accuracy 0.6252 Val acc 0.7086\n",
      "Epoch 0: Loss 0.6340 Val loss 0.5607\n",
      "Epoch 1: Accuracy 0.7771 Val acc 0.7982\n",
      "Epoch 1: Loss 0.4668 Val loss 0.4442\n",
      "Epoch 2: Accuracy 0.8396 Val acc 0.7758\n",
      "Epoch 2: Loss 0.3673 Val loss 0.4582\n",
      "Epoch 3: Accuracy 0.8821 Val acc 0.8436\n",
      "Epoch 3: Loss 0.2864 Val loss 0.3769\n",
      "Epoch 4: Accuracy 0.8807 Val acc 0.8390\n",
      "Epoch 4: Loss 0.2811 Val loss 0.4120\n",
      "Epoch 5: Accuracy 0.9208 Val acc 0.8452\n",
      "Epoch 5: Loss 0.1999 Val loss 0.4333\n",
      "Epoch 6: Accuracy 0.9403 Val acc 0.8542\n",
      "Epoch 6: Loss 0.1569 Val loss 0.3916\n",
      "Epoch 7: Accuracy 0.9570 Val acc 0.8588\n",
      "Epoch 7: Loss 0.1183 Val loss 0.4216\n",
      "Epoch 8: Accuracy 0.9689 Val acc 0.8338\n",
      "Epoch 8: Loss 0.0883 Val loss 0.5147\n",
      "Epoch 9: Accuracy 0.9718 Val acc 0.8582\n",
      "Epoch 9: Loss 0.0808 Val loss 0.5700\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_val, loss_val = evaluate(val_dl)\n",
    "    print(f'Epoch {epoch}: Accuracy {acc_train:.4f} Val acc {acc_val:.4f}')\n",
    "    print(f'Epoch {epoch}: Loss {loss_train:.4f} Val loss {loss_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
